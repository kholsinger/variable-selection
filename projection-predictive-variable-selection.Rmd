---
title: "Projection predictive feature selection"
output: html_notebook
---

https://mc-stan.org/projpred/articles/quickstart.html 

https://arxiv.org/abs/1810.02406

https://link.springer.com/article/10.1007/s11222-016-9649-y

## Setting up the data

As usual, we have to start by generating the data.
```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(reshape2)
library(ggplot2)
library(cowplot)
library(mvtnorm)
library(corrplot)

rm(list = ls())
```

```{r}
## intetcept
##
beta0 <- 1.0
## regression coefficients
##
beta <- c(1.0, -1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)
## pattern of correlation matrix, all non-zero entries are set to saem
## correlation, covariance matrix caldulated from individual variances and a 
## single association parameter governing the non-zero correlation coefficients
##
## Note: Not just any pattern will work here. The correlation matrix and
## covariance matrix generated from this pattern must be positive definite.
## If you change this pattern, you may get an error when you try to generate
## data with a non-zero association parameter.
##
Rho <- matrix(nrow = 9, ncol = , byrow = TRUE, 
              data = c(1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1
                       ))
## vector of standard deviations for covariates
##
sigma <- rep(1, 9)

## construct a covariance matrix from the pattern, standard deviations, and
## one parameter in [-1,1] that governs the magnitude of non-zero correlation
## coefficients
##
## Rho - the pattern of associations
## sigma - the vector of standard deviations
## rho - the association parameter
##
construct_Sigma <- function(Rho, sigma, rho) {
  ## get the correlation matris
  ##
  Rho <- Rho*rho
  for (i in 1:ncol(Rho)) {
    Rho[i,i] <- 1.0
  }
  ## notice the use of matrix multiplication
  ##
  Sigma <- diag(sigma) %*% Rho %*% diag(sigma)
  return(Sigma)
}

## set the random number seed manually so that every run of the code will 
## produce the same numbers
##
set.seed(1234)

n_samp <- 100
cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.8))

resid <- rep(2.0, n_samp)

y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_1 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.8))
y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_2 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

column_names <- c("y", paste("x", seq(1, length(beta)), sep = ""), "Scenario")
colnames(dat_1) <- column_names
colnames(dat_2) <- column_names

## saving results in scale allows me to use them later for prediction with
## new data
##
scale_1 <- lapply(dat_1[, 1:10], scale)
scale_2 <- lapply(dat_2[, 1:10], scale)

## when assigning the same scaling to a data frame, the scaling attributes
## are lost
##
dat_1[, 1:10] <- lapply(dat_1[, 1:10], scale)
dat_2[, 1:10] <- lapply(dat_2[, 1:10], scale)
```

## Trying projection 

Since we'll be running the projection predictive analysis on both data sets, I've written a small function, `projection_prediction()` that will run everything for us and collect the results. Notice that we start by fitting the full model with [horseshoe priors](http://darwin.eeb.uconn.edu/uncommon-ground/blog/2019/09/16/a-bayesian-approach-to-variable-selection-using-horseshoe-priors/).
```{r, warning = FALSE, message = FALSE}
library(rstanarm)
library(projpred)
library(bayesplot)

options(mc.cores = parallel::detectCores())

projection_prediction <- function(dat) {
  n <- nrow(dat)
  D <- ncol(dat[,2:10])
  p0 <- 3
  tau0 <- p0/(D - p0) * 1/sqrt(n)
  prior_coeff <- hs(global_scale = tau0, slab_scale = 1)
  fit <- stan_glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = dat, 
                  prior = prior_coeff, refresh = 0)

  vs <- varsel(fit, method = "forward")
  cvs <- cv_varsel(fit, method = "forward", verbose = FALSE)

  proj_size <- suggest_size(cvs)
  proj <- project(vs, nv = proj_size, ns = 2000)
  mcmc_intervals(as.matrix(proj),
                 pars = c("(Intercept)", names(vs$vind[1:proj_size]), "sigma"))

  pred <- proj_linpred(vs, xnew = dat[, 2:10], ynew = dat$y, 
                       nv = proj_size, integrated = TRUE)
  for.plot <- data.frame(Observed = dat$y,
                         Predicted = pred$pred)
  p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) + 
    geom_point() + 
    geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = "Observed", y = "Predicted")
  print(p)

  return(list(proj = proj, vs = vs, cvs = cvs, proj_size = proj_size))
}

summarize_posterior <- function(x, credible = 0.95, digits = 3) {
  lo_p <- (1.0 - credible)/2.0
  hi_p <- credible + lo_p
  ci <- quantile(x, c(lo_p, hi_p))
  results <- sprintf("% 5.3f (% 5.3f, % 5.3f)\n", mean(x), ci[1], ci[2])
  cat(results, sep = "")
}

summarize_results <- function(x, credible = 0.95, digits = 3) {
  vars <- colnames(as.matrix(x))
  for (i in 1:length(vars)) {
    label <- sprintf("%11s: ", vars[i])
    cat(label, sep = "")
    summarize_posterior(as.matrix(x)[,i])
  }
}

proj_1 <- projection_prediction(dat_1)
proj_2 <- projection_prediction(dat_2)
summarize_results(proj_1$proj)
summarize_results(proj_2$proj)
```
Here we have results that are quite different from one another. Not only is the coefficient on `x3` quite different in the two data sets,[^1] but the variables identified as important are different: `x3` and `x1` in analysis of the first data set and `x3` and `x6` in the second data set. That isn't quite as disturbing as it might first appear. Look back at the results from our regression with horseshoe priors on all of the data. You'll see that `x1` and `x3` had a greater magnitude in analysis of the first data set than any other covariates. Similarly, `x3` and `x6` were the most important in analysis of the second data set.

## Picking the number of covariates

In the code I ran above I used `suggest_size()` to pick the number of covariates to use. If you read the documentation for `suggest_size()`[^2] you'll notice this sentence in the Description:

> It is recommended that the user studies the results via `varsel_plot` and/or `varsel_stats` and makes the final decision based on what is most appropriate for the given problem.

Let's take a look at the results from `varsel_plot()` first.

```{r}
print(varsel_plot(proj_1$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
print(varsel_plot(proj_2$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
```
`elpd` abd `rmse` are measures of how well a model fits the data.[^3] The complete model in our example includes 9 parameters. You can see that in the first data set a model with only 2 covariates works almost as well as the full model, although the model with 3 covariates looks as if it might work a little better. In the second data set, there is clearly no advantage to including more than 2 covariates.

The pictures are pretty clear, but let's look at some the differences numerically using `varsel_stats()`. We'll focus on `rmse`.

```{r}
stats_1 <- varsel_stats(proj_1$cvs) %>% 
  mutate(diff = elpd - last(elpd), percdiff = diff/last(elpd))
stats_2 <- varsel_stats(proj_2$cvs) %>% 
  mutate(diff = elpd - last(elpd), percdiff = diff/last(elpd))

print(round(stats_1, 3))
print(round(stats_2, 3))
```
By default `suggest_size()` identifies the smallest model where `elpd` is within one standard deviation (`elpd.se`) of the full model. In both data sets that happens when two covariates are included. You will notice, however, that just as a model with 3 parameters looked as if it might be a little better than one with only 2 for the first data set,[4] here the model with 3 parameters improves the model with 2 parameters by 0.028, which is more than twice the improvement made by any incuding any additional parameters. This suggests to me that it probably worth looking at the results from the first data set where we include 3 covariates instead of 2.

```{r}
proj_1_3 <- project(proj_1$vs, nv = 3, ns = 2000)
summarize_results(proj_1_3)
```
That's rather interesting. It picked out the "right" coefficients, in the sense that there the ones we set as non-zero in generating the data, and it also got the "right" signs for them. Notice also that the coefficient on `x2` is substantially larger than it is in the analysis of the full model with horseshoe priors (again ignoring the very broad overlap in credible intervals).

We don't have a good reason to look at a model with more than 2 covariates in the second data set, but let's try it with 3 and see what happens.

```{r}
proj_2_3 <- project(proj_2$vs, nv = 3, ns = 2000)
summarize_results(proj_2_3)
```
That did bring in `x1`, which we know ought to be there. But again, the only reason we have to prefer this model given these data is our prior knowledge that `x3` ought to be in the model. 

## Conclusions

If you can run a regression model in `stan_glm()` or `stan_glmer()`, you can easily use horseshoe priors. And if you can do that, you can easily use projection predictive variable selection to identtify the "most important" set of covariates. The simple example here suggests a couple of things:

1. Be sure to examine `varsel_plot()`, `varsel_stats()`, or both rather than simply relying on `suggest_size()` to tell you how many covariates to include.

2. It's probably worth your time to take a look at models that include one or two more parameters than what your examination of `varsel_plot()` and `varsel_stats()` suggest. You'll need to bring in your subject matter knowledge here,[^5] but there might be a covariate with an association worth considering based on subtantive grounds, even if it's not "significant" in a conventional sense.

That last point raises a much broader issue that I'll return to in the last blog post in this series, where I try to summarize the lessons we've learned and provide some general guidelines. The difference between "significant" and "not Significant" is not itself statistically significant.[^6]

[^1]: I'm being sloppy here. I should have said "the _posterior means_ for `x3` is quite different, but the posterior distribution overlaps quite broadly."

[^2]: You should always, _always_, _**always**_ read the documentation.

[^3]: `rmse` is the easiest to understand. It's the "root mean squared error", i.e., the square root of the average squared error (squared residual) across all observations. If $x_i$ is the $i$th observation and $\mu_i$ is the prediction for that observation, then 
$$
rmse = \sqrt{\sum_i (x_i - \mu_i)^2} \quad .
$$
`rmse` is an appropriate measure only for models where the residual variance is normally distributed. `elpd` is a more general measure that can be used with any model that `rstanarm` fits.

[^4]: So long as you pay attention only to the posterior means and not their standard errors.

[^5]: Since you won't know the truth as we did here.

[^6]: That's the title of a paper by Andrew Gelman and Hal Stern in _The American Statistician_. doi: [10.1198/000313006X152649](https://doi.org/10.1198/000313006X152649)