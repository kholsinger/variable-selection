---
title: "Do we do better when covariates are less associated?"
output: 
  html_notebook:
    toc: true
    toc_float: 
      collapsed: false
---
## Introduction

Unlike previous notebooks, there isn't anything new here. What you'll see instead is analysis of two data sets generated with less association among the covariates than the data sets you've seen in earlier notebooks. The analysis will use `rstanarm`, horseshoe priors, and projection prediction variable selection.

NOTE: Data the data not scaled so that regression coefficients are _more_ comparable across the two data sets. You may want to return to some of the earlier notebooks where the data were scaled and re-run the analyses to see how the results change.

```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(reshape2)
library(ggplot2)
library(cowplot)
library(mvtnorm)
library(corrplot)

rm(list = ls())
```

```{r}
## intetcept
##
beta0 <- 1.0
## regression coefficients
##
beta <- c(1.0, -1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)
## pattern of correlation matrix, all non-zero entries are set to saem
## correlation, covariance matrix caldulated from individual variances and a 
## single association parameter governing the non-zero correlation coefficients
##
## Note: Not just any pattern will work here. The correlation matrix and
## covariance matrix generated from this pattern must be positive definite.
## If you change this pattern, you may get an error when you try to generate
## data with a non-zero association parameter.
##
Rho <- matrix(nrow = 9, ncol = , byrow = TRUE, 
              data = c(1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1,
                       0,1,0,1,0,1,0,1,0,
                       1,0,1,0,1,0,1,0,1
                       ))
## vector of standard deviations for covariates
##
sigma <- rep(1, 9)

## construct a covariance matrix from the pattern, standard deviations, and
## one parameter in [-1,1] that governs the magnitude of non-zero correlation
## coefficients
##
## Rho - the pattern of associations
## sigma - the vector of standard deviations
## rho - the association parameter
##
construct_Sigma <- function(Rho, sigma, rho) {
  ## get the correlation matris
  ##
  Rho <- Rho*rho
  for (i in 1:ncol(Rho)) {
    Rho[i,i] <- 1.0
  }
  ## notice the use of matrix multiplication
  ##
  Sigma <- diag(sigma) %*% Rho %*% diag(sigma)
  return(Sigma)
}

## set the random number seed manually so that every run of the code will 
## produce the same numbers
##
set.seed(1234)

n_samp <- 100
cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.8))

resid <- rep(2.0, n_samp)

y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_1 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.8))
y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_2 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

column_names <- c("y", paste("x", seq(1, length(beta)), sep = ""), "Scenario")
colnames(dat_1) <- column_names
colnames(dat_2) <- column_names
```

### Correlation plots with `rho = 0.8`

```{r}
corrplot(cor(dat_1[, -c(1,11)]))
corrplot(cor(dat_2[, -c(1,11)]))
corr_08_1 <- max(abs(cor(dat_1[, -c(1,11)]) - diag(1, 9)))
corr_08_2 <- max(abs(cor(dat_2[, -c(1,11)]) - diag(1, 9)))
```

### Correlation plots with `rho = 0.2`

```{r}
n_samp <- 100
cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.2))

resid <- rep(2.0, n_samp)

y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_1 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

cov_str <- rmvnorm(n_samp,
                   mean = rep(0, nrow(Rho)),
                   sigma = construct_Sigma(Rho, sigma, 0.2))
y_str <- rnorm(nrow(cov_str), mean = beta0 + cov_str %*% beta, sd = resid)
dat_2 <- data.frame(y_str, cov_str, rep("Strong", length(y_str)))

column_names <- c("y", paste("x", seq(1, length(beta)), sep = ""), "Scenario")
colnames(dat_1) <- column_names
colnames(dat_2) <- column_names

corrplot(cor(dat_1[, -c(1,11)]))
corrplot(cor(dat_2[, -c(1,11)]))
corr_02_1 <- max(abs(cor(dat_1[, -c(1,11)]) - diag(1, 9)))
corr_02_2 <- max(abs(cor(dat_2[, -c(1,11)]) - diag(1, 9)))
```

The correlation plots make it clear that the magnitude of associations among covariates is much greater with `rho = 0.8` than it is with `rho = 0.2`, but let's look at the numerical results.

The maximum correlation in data set 1 (rho = 0.8): `r round(corr_08_1, 3)`

The maximum correlation in data set 2 (rho = 0.8): `r round(corr_08_2, 3)`

The maximum correlation in data set 1 (rho = 0.2): `r round(corr_02_1, 3)`

The maximum correlation in data set 2 (rho = 0.2): `r round(corr_02_2, 3)`

In everything that follows, we'll be working with the data sets generated when `rho = 0.2`.

## Horseshoe priors

We'll be working with the data set generated with relatively weak associations among covariates for the rest of this exercise, and we'll start with horsehoe priors.[^1]

```{r, warning = FALSE, message = FALSE}
library(rstanarm)

options(mc.cores = parallel::detectCores())

set.seed(1234)

## n is the number of observations
## D is the number of covariates
## p0 is the expected number of important covariates
##
n <- nrow(dat_1)
D <- ncol(dat_1[,2:10])
p0 <- 3
tau0 <- p0/(D - p0) * 1/sqrt(n)
prior_coeff <- hs(global_scale = tau0, slab_scale = 1)

fit_1 <- stan_glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9,
                  data = dat_1,
                  prior = prior_coeff,
                  refresh = 0,
                  adapt_delta = 0.999)
## Note: I use the same prior scale here because both data sets have the same number of observations
## and the same expected number of important covariates
##
fit_2 <- stan_glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9,
                  data = dat_2,
                  prior = prior_coeff,
                  refresh = 0,
                  adapt_delta = 0.999)

predict_1 <- posterior_predict(fit_1)
predict_2 <- posterior_predict(fit_2)

for.plot <- data.frame(Observed = dat_1$y, Predicted = apply(predict_1, 2, mean))
p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm") +
  ggtitle("Data set 1")
print(p)

for.plot <- data.frame(Observed = dat_2$y, Predicted = apply(predict_2, 2, mean))
p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm") +
  ggtitle("Data set 2")
print(p)

p <- plot(fit_1) + ggtitle("Estimates from data set 1")
print(p)
p <- plot(fit_2) + ggtitle("Estimates from data set 2")
print(p)

summary(fit_1, digits = 3)
summary(fit_2, digits = 3)

new_data <- data.frame(x1 = 4.0, x2 = 4.0, x3 = 4.0,
                       x4 = 4.0, x5 = 4.0, x6 = 4.0,
                       x7 = 4.0, x8 = 4.0, x9 = 4.0)

predict_1 <- posterior_predict(fit_1, new_data)
predict_2 <- posterior_predict(fit_2, new_data)

summarize_posterior <- function(x, credible = 0.95, digits = 3) {
  lo_p <- (1.0 - credible)/2.0
  hi_p <- credible + lo_p
  ci <- quantile(x, c(lo_p, hi_p))
  cat(round(mean(x), 3), " (", round(ci[1], 3), ",", round(ci[2], 3), ")\n", sep = "")
}

summarize_posterior(predict_1)
cat("  True answer: ", beta0 + as.matrix(new_data) %*% beta, "\n", 
    sep = "")
summarize_posterior(predict_2)
cat("  True answer: ", beta0 + as.matrix(new_data) %*% beta, "\n", 
    sep = "")
```

That looks pretty encouraging. In both cases, `x1`, `x2`, and `x3` are picked out as being important, and the estimated coefficients for each are relatively close to the values used to generate the data. The evidence for an association involving any of the other covariates is pretty weak.

## Projection prediction

Now let's try projection prediction to see how many variables we'd include in a model and which ones they are if we wanted explicitly to simplify the model rather than directly interpreting coefficients of the full model. 

```{r, warning = FALSE, message = FALSE}
library(rstanarm)
library(projpred)
library(bayesplot)

options(mc.cores = parallel::detectCores())

projection_prediction <- function(dat) {
  n <- nrow(dat)
  D <- ncol(dat[,2:10])
  p0 <- 3
  tau0 <- p0/(D - p0) * 1/sqrt(n)
  prior_coeff <- hs(global_scale = tau0, slab_scale = 1)
  fit <- stan_glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = dat, 
                  prior = prior_coeff, refresh = 0)

  vs <- varsel(fit, method = "forward")
  cvs <- cv_varsel(fit, method = "forward", verbose = FALSE)

  proj_size <- suggest_size(cvs)
  proj <- project(vs, nv = proj_size, ns = 2000)
  mcmc_intervals(as.matrix(proj),
                 pars = c("(Intercept)", names(vs$vind[1:proj_size]), "sigma"))

  pred <- proj_linpred(vs, xnew = dat[, 2:10], ynew = dat$y, 
                       nv = proj_size, integrated = TRUE)
  for.plot <- data.frame(Observed = dat$y,
                         Predicted = pred$pred)
  p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) + 
    geom_point() + 
    geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = "Observed", y = "Predicted")
  print(p)

  return(list(proj = proj, vs = vs, cvs = cvs, proj_size = proj_size))
}

summarize_posterior <- function(x, credible = 0.95, digits = 3) {
  lo_p <- (1.0 - credible)/2.0
  hi_p <- credible + lo_p
  ci <- quantile(x, c(lo_p, hi_p))
  results <- sprintf("% 5.3f (% 5.3f, % 5.3f)\n", mean(x), ci[1], ci[2])
  cat(results, sep = "")
}

summarize_results <- function(x, credible = 0.95, digits = 3) {
  vars <- colnames(as.matrix(x))
  for (i in 1:length(vars)) {
    label <- sprintf("%11s: ", vars[i])
    cat(label, sep = "")
    summarize_posterior(as.matrix(x)[,i])
  }
}

proj_1 <- projection_prediction(dat_1)
proj_2 <- projection_prediction(dat_2)

print(varsel_plot(proj_1$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
print(varsel_plot(proj_2$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
summarize_results(proj_1$proj)
summarize_results(proj_2$proj)
```

Good news! Using projection prediction to select the "important" covariates identified the same three covariates our examination of the full model suggested to us _*and*_ those are the variables we know ore important.

Bottom line: If associations among your covariates are modest, if you have a reasonably large amount of data, _*and if your covariates include all of those that have a real relationship with the response variable*_, then you have a good chance of detecting the relationships and of estimating their magnitude. But what happens if the "true" predictors are among the covariates included in your study?

## What if the "true" predictors aren't in the data?

In our case the true predictors are `x1`, `x2`, and `x3`. We know that they influence influence `y` and that no other covariates do, because we generated the data that way. What happens if we fit a model that excludes `x1`, `x2`, and `x3` and includes only covariates that we know don't have a "real" relationship to `y`?

```{r}
n <- nrow(dat_1)
D <- ncol(dat_1[,5:10])
p0 <- 3
tau0 <- p0/(D - p0) * 1/sqrt(n)
prior_coeff <- hs(global_scale = tau0, slab_scale = 1)

fit_1 <- stan_glm(y ~ x4 + x5 + x6 + x7 + x8 + x9,
                  data = dat_1,
                  prior = prior_coeff,
                  refresh = 0,
                  adapt_delta = 0.999)
## Note: I use the same prior scale here because both data sets have the same number of observations
## and the same expected number of important covariates
##
fit_2 <- stan_glm(y ~ x4 + x5 + x6 + x7 + x8 + x9,
                  data = dat_2,
                  prior = prior_coeff,
                  refresh = 0,
                  adapt_delta = 0.999)

predict_1 <- posterior_predict(fit_1)
predict_2 <- posterior_predict(fit_2)

for.plot <- data.frame(Observed = dat_1$y, Predicted = apply(predict_1, 2, mean))
p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm") +
  ggtitle("Data set 1")
print(p)

for.plot <- data.frame(Observed = dat_2$y, Predicted = apply(predict_2, 2, mean))
p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm") +
  ggtitle("Data set 2")
print(p)

p <- plot(fit_1) + ggtitle("Estimates from data set 1")
print(p)
p <- plot(fit_2) + ggtitle("Estimates from data set 2")
print(p)

summary(fit_1, digits = 3)
summary(fit_2, digits = 3)

new_data <- data.frame(x4 = 4.0, x5 = 4.0, x6 = 4.0,
                       x7 = 4.0, x8 = 4.0, x9 = 4.0)

predict_1 <- posterior_predict(fit_1, new_data)
predict_2 <- posterior_predict(fit_2, new_data)

summarize_posterior <- function(x, credible = 0.95, digits = 3) {
  lo_p <- (1.0 - credible)/2.0
  hi_p <- credible + lo_p
  ci <- quantile(x, c(lo_p, hi_p))
  cat(round(mean(x), 3), " (", round(ci[1], 3), ",", round(ci[2], 3), ")\n", sep = "")
}

summarize_posterior(predict_1)
summarize_posterior(predict_2)
```

The results from analysis of data set 1 suggest (weakly) that `x4` and `x8` have a negative association with `y`, and that `x9` has a positive associations. That's right in the sense that (a) `x4` and `x8` are positively associated with `x2`, which we know has a negative influence on `y` and (b) `x9` is positively associated with both `x1` and `x3`, which we know have positive influences on `y`. In data set 2, `x4` and `x6` have a weakly supported negative association with `y`, while `x5` has more strongly supported positive association with `y`. Again, those associations are in the direction we'd expect. But the associations are weak. Our evidence for them is shaky.

What happens if we try the projection predictions method for variable selection?

```{r}
projection_prediction <- function(dat) {
  n <- nrow(dat)
  D <- ncol(dat) - 1
  p0 <- 3
  tau0 <- p0/(D - p0) * 1/sqrt(n)
  prior_coeff <- hs(global_scale = tau0, slab_scale = 1)
  fit <- stan_glm(y ~ x4 + x5 + x6 + x7 + x8 + x9, data = dat, 
                  prior = prior_coeff, refresh = 0)

  vs <- varsel(fit, method = "forward")
  cvs <- cv_varsel(fit, method = "forward", verbose = FALSE)

  proj_size <- suggest_size(cvs)
  proj <- project(vs, nv = proj_size, ns = 2000)
  mcmc_intervals(as.matrix(proj))

  pred <- proj_linpred(vs, xnew = dat[, -1], ynew = dat$y, 
                       nv = proj_size, integrated = TRUE)
  for.plot <- data.frame(Observed = dat$y,
                         Predicted = pred$pred)
  p <- ggplot(for.plot, aes(x = Observed, y = Predicted)) + 
    geom_point() + 
    geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = "Observed", y = "Predicted")
  print(p)

  return(list(proj = proj, vs = vs, cvs = cvs, proj_size = proj_size))
}

proj_1 <- projection_prediction(dat_1[, -c(2,3,4,11)])
proj_2 <- projection_prediction(dat_2[, -c(2,3,4,11)])

print(varsel_plot(proj_1$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
print(varsel_plot(proj_2$cvs, stats = c("elpd", "rmse"), deltas = TRUE))
summarize_results(proj_1$proj)
summarize_results(proj_2$proj)
```

I said that the data set 1 suggested some relationships weakly, and the projection predictions show that the relationships are so weak that we don't have good evidence for including any of them. There's reasonable evidence for including one covariate, `x5`, in the second data set, which means we don't have any indication of any negative associations with `y`.[^2]

## Conclusions

If associations among our covariates are relatively weak and we have a reasonable amount of data, it's tempting to think that associations we detect in our data are likely to be real. It's certainly the case that we're less likely to be misled when associations are wake, but try this simple thought experiment to realize why you should be cautious.

Imagine that you're interested in how leaf mass per area (LMA) varies across environmental gradients. Suppose you're working in a mountainous region of the North Temperate Zone and that you include elevation as a covariate without including any covariates related to temperature (e.g., mean annual temperature, January minimum temperature, number of frost-free days). I suspect you;ll find a positive association between LMA and elevation, but I seriously doubt that it's elevation _per se_ that's important. Rather, I suspect that the relationship, assuming that it exists, would reflect the tight association between different aspects of temperature and elevation. Even if you included a temperature covariate, e.g., frost-free days, and you now found an association between LMA and frost-free days while the elevation association was now weak or non-existent, I'd suggest that you think very carefully before concluding that frost-free days _*really*_ matters. It might be one of several temperature-related covariates that matter, or it might simply be tightly associated with something else that really matters. In the absence of a controlled experiment or a very careful causal analysis,{^3} all we can really say from the data alone is that there's an association.  
In short, I urge you to be very cautious in interpreting the results of a multiple regression. If the associations you identify have been found repeatedly in other data sets and there are good, principled reasons to believe that those associations arose through the process you are trying to attribute them to, I'd say you're on reasonably solid ground. The less often the associations you see have been seen before and the less they might have been predicted from first principles, the shakier is the ground on which you stand. But being cautious in interpreting the meaning of associations you find, doesn't mean that those associations are unimportant. Associations that appear important in your analysis are worth further study, and that further study may lead both to finding them repeatedly and developing an understanding of mechanisms and processes that could produce them.

[^1]: You should find it pretty easy to try the Lasso using the least squares regression in `lm()` if you're so inclined.

{^2]: I havern't tried running the models that exclude `x1`, `x2`, and `x3` with the strong associations (0.8 in the call to `rmvnorm()`), but I encourage you to try it on your own. I suspect you'll find some reasonably well supported associations if you do.